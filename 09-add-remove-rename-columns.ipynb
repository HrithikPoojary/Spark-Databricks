{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22e07ae-e91a-4e70-9ae7-625b23b8a803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    (100,'Prashant' , 45 ,45000),\n",
    "    (101,'Tarun' ,36 , 33000),\n",
    "    (102,'David' , 48 ,28000)\n",
    "]\n",
    "schema = 'id int , name string , age short , salary double'\n",
    "\n",
    "no_col = spark.createDataFrame(data = data_list )\n",
    "sample_todf = spark.createDataFrame(data = data_list).toDF(\"id\" , \"name\" , \"age\" , \"salary\")\n",
    "# best practice\n",
    "sample_df = spark.createDataFrame(data = data_list , schema = schema)\n",
    "\n",
    "no_col.printSchema()\n",
    "sample_todf.printSchema()\n",
    "print(\"Best Practice\")\n",
    "sample_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769320b2-0324-4dad-908d-a5677d8e0b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "sample_df.withColumns(\n",
    "    {\n",
    "        \"increment\" : expr(\" case when salary > 30000 then 3000 else  (salary * 10) / 100 end  \"),\n",
    "        \"revised_salary\" : expr(\" increment + salary\")\n",
    "    }\n",
    ").display() \n",
    "#-- \"revised_salary\" : expr(\" increment + salary\")\n",
    "#                             increment column we can use only when new column creation not on the old column modification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ef7f7e-1c79-45e4-8eae-ad5e5368d508",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "sample_df.withColumns(\n",
    "    {\n",
    "        \"increment\" : expr(\" case when salary > 30000 then 3000 else  (salary * 10) / 100 end  \"),\n",
    "        \"salary\" : expr(\" case when salary > 30000 then 3000 else  (salary * 10) / 100 end + salary\")\n",
    "    }\n",
    ").display() \n",
    "# we cannot modify the old column here\n",
    "\n",
    "salary_df = sample_df.withColumn(\"increment\" , expr( \" case when salary > 30000 then 3000 else (salary * 10)/100 end\"))\\\n",
    "              .withColumn( \"salary\" ,    expr(\" salary + increment \"))\n",
    "salary_df.display()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1db3824-960d-43b7-9cf7-eeab76be9651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To assing UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4996fe60-fd64-48f8-9f06-ec072e978733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "batch_id = str(uuid.uuid4()) #spark does,nt support uuid type\n",
    "\n",
    "batch_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f259a67-852b-44ed-8662-8fa88d52b3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df.withColumn(\"batch_id\" , expr(f\"{batch_id}\"))  # error\n",
    "# expr - expr is used only when columns  , sql like statement and some literals (let()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef30a92-dd65-4283-9a0b-f55198cc9db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# lit() - > this will convert to column type value \n",
    "sample_df.withColumn(\"batch_id\" , lit(batch_id)).display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535a1ba1-2048-4d3a-a155-e5c3470a675b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "salary_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec593c36-bfbf-49b6-93f8-1464f533bd63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1a0b09-bc41-4755-b6ef-8ae7bab32dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "salary_df.withColumnsRenamed(\n",
    "    {\n",
    "        \"increment\" : \"annuall_increment\",\n",
    "        \"salary\" : \"increment_salary\"\n",
    "    }\n",
    ").display()\n",
    "# withColumnsRenamed = for multiple columns\n",
    "# withColumnRenamed = for single column\n",
    "\n",
    "salary_df.withColumnRenamed( \"increment\" , \"annuall_increment\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "110d8efa-8b50-42d9-8cf7-5ba71a40ab4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f2a093-dcdf-4101-8d87-d7100b74000a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df.drop(\"id\" , \"salary\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09-add-remove-rename-columns",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
