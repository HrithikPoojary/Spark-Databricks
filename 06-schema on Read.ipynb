{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44f379a-c8dc-41b0-afcc-49ac625e0d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\")\\\n",
    "               .option('mode' , 'FAILFAST')\\\n",
    "                .option('dateFormat' , 'M/d/yyyy')\\\n",
    "                .load(path = '/Volumes/dev/spark_db/datasets/spark_programming/data/flight-time.json')\n",
    "\n",
    "# mode = PERMISSIVE - > skips the corrupted and keep a track of the data\n",
    "#      = DROPMALFORMED - > ignores the whole corrupted records\n",
    "#      = FAILFAST     - > throws an exception when it meets corrupted records.\n",
    "\n",
    "# dataFormat - > data present in the json file 1/1/2000\n",
    "#                                              M/d/yyyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a72e71-3c63-4b23-be07-971c2afff0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "\n",
    "#Two problems\n",
    "#    1) Order of the schema (columns)\n",
    "#    2) dateFormat - > not able find date type correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e0acfb-efe3-4de2-a40a-f6d3727661bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(3).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "067538e9-a256-4c50-8aa5-51522913f241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "   FL_DATE DATE, \n",
    "    OP_CARRIER STRING, \n",
    "    OP_CARRIER_FL_NUM STRING, \n",
    "    ORIGIN STRING, \n",
    "    ORIGIN_CITY_NAME STRING, \n",
    "    DEST STRING, \n",
    "    DEST_CITY_NAME STRING, \n",
    "    CRS_DEP_TIME LONG, \n",
    "    DEP_TIME LONG, \n",
    "    WHEELS_ON INT, \n",
    "    TAXI_IN INT, \n",
    "    CRS_ARR_TIME LONG, \n",
    "    ARR_TIME LONG, \n",
    "    CANCELLED INT, \n",
    "    DISTANCE INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c34324-fb5e-4600-b27e-76ad3fda1ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will increase performance\n",
    "# Nomal schema it has check all the data to fix the datatype of all data\n",
    "# using schema() we will use predefined schema no need to check the data\n",
    "\n",
    "from pyspark.sql.types import StructType , StructField , StringType , IntegerType , DateType , LongType\n",
    "\n",
    "flight_schema = StructType(\n",
    "\n",
    "        [\n",
    "            StructField (   'FL_DATE' ,  DateType()), \n",
    "            StructField (    'OP_CARRIER', StringType()), \n",
    "            StructField (    'OP_CARRIER_FL_NUM', StringType()), \n",
    "            StructField (    'ORIGIN' ,StringType()), \n",
    "            StructField (     \"ORIGIN_CITY_NAME\" ,StringType()), \n",
    "            StructField (     \"DEST\" ,StringType()),\n",
    "            StructField (     \"DEST_CITY_NAME\" ,StringType()),\n",
    "            StructField (     \"CRS_DEP_TIME\", LongType()), \n",
    "            StructField (     \"DEP_TIME\", LongType()), \n",
    "            StructField (     \"WHEELS_ON\", IntegerType()), \n",
    "            StructField (     \"TAXI_IN\",  IntegerType()), \n",
    "            StructField (     \"CRS_ARR_TIME\",LongType()), \n",
    "            StructField (     \"ARR_TIME\",LongType()), \n",
    "            StructField (     \"CANCELLED\", IntegerType()), \n",
    "            StructField (     \"DISTANCE\",  IntegerType())\n",
    "        ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64dbad6-5e9a-43ae-bf21-6c96c9bdbc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_on_schema = spark.read.format(\"json\")\\\n",
    "                .option('mode' , 'FAILFAST')\\\n",
    "                .option('dateFormat' , 'M/d/yyyy')\\\n",
    "                .schema(flight_schema)\\\n",
    "                .load(path = '/Volumes/dev/spark_db/datasets/spark_programming/data/flight-time.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a587d9f-52b5-46d5-ae35-284d5f65a402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_on_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5def2ed4-e04f-47ac-8088-097e73d83199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_on_schema.limit(3).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc61dad-60af-491d-b53b-211d6445b566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_on_schema.write.mode('overwrite').saveAsTable(\"dev.spark_db.flight_time_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3c521d-40bf-42cc-9135-caa9dd0f5b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from dev.spark_db.flight_time_raw limit 5\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06-schema on Read",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
